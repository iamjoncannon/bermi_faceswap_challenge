These are the help outputs from the CLI


///////////

usage: tools.py effmpeg [-h] [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE]
                        [-a {extract,gen-vid,get-fps,get-info,mux-audio,rescale,rotate,slice}]
                        -i INPUT [-o OUTPUT] [-r REF_VID] [-fps FPS]
                        [-ef {.bmp,.jpeg,.jpg,.png,.tif,.tiff}] [-s START]
                        [-e END] [-d DURATION] [-m]
                        [-tr {0, 90CounterClockwise&VerticalFlip),(1, 90Clockwise),(2, 90CounterClockwise),(3, 90Clockwise&VerticalFlip}]
                        [-de DEGREES] [-sc SCALE] [-pr] [-q] [-v]

This command allows you to easily execute common ffmpeg tasks.

optional arguments:
  -h, --help            show this help message and exit
  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}
                        Log level. Stick with INFO or VERBOSE unless you need
                        to file an error report. Be careful with TRACE as it
                        will generate a lot of data
  -LF LOGFILE, --logfile LOGFILE
                        Path to store the logfile. Leave blank to store in the
                        faceswap folder
  -a {extract,gen-vid,get-fps,get-info,mux-audio,rescale,rotate,slice}, --action {extract,gen-vid,get-fps,get-info,mux-audio,rescale,rotate,slice}
                        Choose which action you want ffmpeg ffmpeg to do.
                          - 'slice' cuts a portion of the video into a
                            separate video file.
                          - 'get-fps' returns the chosen video's fps.
  -i INPUT, --input INPUT
                        Input file.
  -o OUTPUT, --output OUTPUT
                        Output file. If no output is specified then: if the
                        output is meant to be a video then a video called
                        'out.mkv' will be created in the input directory; if
                        the output is meant to be a directory then a directory
                        called 'out' will be created inside the input
                        directory.Note: the chosen output file extension will
                        determine the file encoding.
  -r REF_VID, --reference-video REF_VID
                        Path to reference video if 'input' was not a video.
  -fps FPS, --fps FPS   Provide video fps. Can be an integer, float or
                        fraction. Negative values will make the program try to
                        get the fps from the input or reference videos.
  -ef {.bmp,.jpeg,.jpg,.png,.tif,.tiff}, --extract-filetype {.bmp,.jpeg,.jpg,.png,.tif,.tiff}
                        Image format that extracted images should be saved as.
                        '.bmp' will offer the fastest extraction speed, but
                        will take the most storage space. '.png' will be
                        slower but will take less storage.
  -s START, --start START
                        Enter the start time from which an action is to be
                        applied. Default: 00:00:00, in HH:MM:SS format. You
                        can also enter the time with or without the colons,
                        e.g. 00:0000 or 026010.
  -e END, --end END     Enter the end time to which an action is to be
                        applied. If both an end time and duration are set,
                        then the end time will be used and the duration will
                        be ignored. Default: 00:00:00, in HH:MM:SS.
  -d DURATION, --duration DURATION
                        Enter the duration of the chosen action, for example
                        if you enter 00:00:10 for slice, then the first 10
                        seconds after and including the start time will be cut
                        out into a new video. Default: 00:00:00, in HH:MM:SS
                        format. You can also enter the time with or without
                        the colons, e.g. 00:0000 or 026010.
  -m, --mux-audio       Mux the audio from the reference video into the input
                        video. This option is only used for the 'gen-vid'
                        action. 'mux-audio' action has this turned on
                        implicitly.
  -tr {(0, 90CounterClockwise&VerticalFlip),(1, 90Clockwise),(2, 90CounterClockwise),(3, 90Clockwise&VerticalFlip)}, --transpose {(0, 90CounterClockwise&VerticalFlip),(1, 90Clockwise),(2, 90CounterClockwise),(3, 90Clockwise&VerticalFlip)}
                        Transpose the video. If transpose is set, then degrees
                        will be ignored. For cli you can enter either the
                        number or the long command name, e.g. to use (1,
                        90Clockwise) -tr 1 or -tr 90Clockwise
  -de DEGREES, --degrees DEGREES
                        Rotate the video clockwise by the given number of
                        degrees.
  -sc SCALE, --scale SCALE
                        Set the new resolution scale if the chosen action is
                        'rescale'.
  -pr, --preview        Uses ffplay to preview the effects of actions that
                        have a video output. Currently preview does not work
                        when muxing audio.
  -q, --quiet           Reduces output verbosity so that only serious errors
                        are printed. If both quiet and verbose are set,
                        verbose will override quiet.
  -v, --verbose         Increases output verbosity. If both quiet and verbose
                        are set, verbose will override quiet.

Questions and feedback: https://github.com/deepfakes/faceswap-playground







////////////

usage: faceswap.py extract [-h] [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE]
                           -i INPUT_DIR -o OUTPUT_DIR [-al ALIGNMENTS_PATH]
                           [-l REF_THRESHOLD] [-n NFILTER [NFILTER ...]]
                           [-f FILTER [FILTER ...]]
                           [--serializer {json,pickle,yaml}]
                           [-D {cv2-dnn,mtcnn,s3fd}] [-A {dlib,fan}]
                           [-r ROTATE_IMAGES] [-bt BLUR_THRESH] [-mp]
                           [-sz SIZE] [-min MIN_SIZE] [-een EXTRACT_EVERY_N]
                           [-s] [-sf] [-dl] [-ae] [-si SAVE_INTERVAL]

Extract the faces from pictures

optional arguments:
  -h, --help            show this help message and exit
  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}
                        Log level. Stick with INFO or VERBOSE unless you need
                        to file an error report. Be careful with TRACE as it
                        will generate a lot of data
  -LF LOGFILE, --logfile LOGFILE
                        Path to store the logfile. Leave blank to store in the
                        faceswap folder
  -i INPUT_DIR, --input-dir INPUT_DIR
                        Input directory or video. Either a directory
                        containing the image files you wish to process or path
                        to a video file.
  -o OUTPUT_DIR, --output-dir OUTPUT_DIR
                        Output directory. This is where the converted files
                        will be saved.
  -al ALIGNMENTS_PATH, --alignments ALIGNMENTS_PATH
                        Optional path to an alignments file.
  -l REF_THRESHOLD, --ref_threshold REF_THRESHOLD
                        Threshold for positive face recognition. For use with
                        nfilter or filter. Lower values are stricter.
  -n NFILTER [NFILTER ...], --nfilter NFILTER [NFILTER ...]
                        Reference image for the persons you do not want to
                        process. Should be a front portrait. Multiple images
                        can be added space separated
  -f FILTER [FILTER ...], --filter FILTER [FILTER ...]
                        Reference images for the person you want to process.
                        Should be a front portrait. Multiple images can be
                        added space separated
  --serializer {json,pickle,yaml}
                        Serializer for alignments file. If yaml is chosen and
                        not available, then json will be used as the default
                        fallback.
  -D {cv2-dnn,mtcnn,s3fd}, --detector {cv2-dnn,mtcnn,s3fd}
                        Detector to use. Some of these have configurable
                        settings in '/config/extract.ini' or 'Edit > Configure
                        Extract Plugins':
                          - 'cv2-dnn': A CPU only extractor, is the least
                            reliable, but uses least resources and runs fast
                            on CPU. Use this if not using a GPU and time is
                            important.
                          - 'mtcnn': Fast on GPU, slow on CPU. Uses fewer
                            resources than other GPU detectors but can often
                            return more false positives.
                          - 's3fd': Fast on GPU, slow on CPU. Can detect more
                            faces and fewer false positives than other GPU
                            detectors, but is a lot more resource intensive.
  -A {dlib,fan}, --aligner {dlib,fan}
                        Aligner to use.
                          - 'dlib': Dlib Pose Predictor. Faster, less resource
                            intensive, but less accurate.
                          - 'fan': Face Alignment Network. Best aligner. GPU
                            heavy, slow when not running on GPU
  -r ROTATE_IMAGES, --rotate-images ROTATE_IMAGES
                        If a face isn't found, rotate the images to try to
                        find a face. Can find more faces at the cost of
                        extraction speed. Pass in a single number to use
                        increments of that size up to 360, or pass in a list
                        of numbers to enumerate exactly what angles to check
  -bt BLUR_THRESH, --blur-threshold BLUR_THRESH
                        Automatically discard images blurrier than the
                        specified threshold. Discarded images are moved into a
                        "blurry" sub-folder. Lower values allow more blur. Set
                        to 0.0 to turn off.
  -mp, --multiprocess   Run extraction in parallel. Offers speed up for some
                        extractor/detector combinations, less so for others.
                        Only has an effect if both the aligner and detector
                        use the GPU, otherwise this is automatic.
  -sz SIZE, --size SIZE
                        The output size of extracted faces. Make sure that the
                        model you intend to train supports your required size.
                        This will only need to be changed for hi-res models.
  -min MIN_SIZE, --min-size MIN_SIZE
                        Filters out faces detected below this size. Length, in
                        pixels across the diagonal of the bounding box. Set to
                        0 for off
  -een EXTRACT_EVERY_N, --extract-every-n EXTRACT_EVERY_N
                        Extract every 'nth' frame. This option will skip
                        frames when extracting faces. For example a value of 1
                        will extract faces from every frame, a value of 10
                        will extract faces from every 10th frame.
  -s, --skip-existing   Skips frames that have already been extracted and
                        exist in the alignments file
  -sf, --skip-existing-faces
                        Skip frames that already have detected faces in the
                        alignments file
  -dl, --debug-landmarks
                        Draw landmarks on the ouput faces for debug
  -ae, --align-eyes     Perform extra alignment to ensure left/right eyes are
                        at the same height
  -si SAVE_INTERVAL, --save-interval SAVE_INTERVAL
                        Automatically save the alignments file after a set
                        amount of frames. Will only save at the end of
                        extracting by default. WARNING: Don't interrupt the
                        script when writing the file because it might get
                        corrupted. Set to 0 to turn off





///////////////

train help

usage: faceswap.py train [-h] [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE] -A
                         INPUT_A -B INPUT_B [-ala ALIGNMENTS_PATH_A]
                         [-alb ALIGNMENTS_PATH_B] -m MODEL_DIR
                         [-t {dfaker,dfl-h128,iae,lightweight,original,realface,unbalanced,villain}]
                         [-s SAVE_INTERVAL] [-bs BATCH_SIZE] [-it ITERATIONS]
                         [-g GPUS] [-ps PREVIEW_SCALE] [-p] [-w] [-ag] [-nl]
                         [-pp] [-msg] [-wl] [-nf] [-tia TIMELAPSE_INPUT_A]
                         [-tib TIMELAPSE_INPUT_B] [-to TIMELAPSE_OUTPUT]

This command trains the model for the two faces A and B

optional arguments:
  -h, --help            show this help message and exit
  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}
                        Log level. Stick with INFO or VERBOSE unless you need
                        to file an error report. Be careful with TRACE as it
                        will generate a lot of data
  -LF LOGFILE, --logfile LOGFILE
                        Path to store the logfile. Leave blank to store in the
                        faceswap folder
  -A INPUT_A, --input-A INPUT_A
                        Input directory. A directory containing training
                        images for face A.
  -B INPUT_B, --input-B INPUT_B
                        Input directory. A directory containing training
                        images for face B.
  -ala ALIGNMENTS_PATH_A, --alignments-A ALIGNMENTS_PATH_A
                        Path to alignments file for training set A. Only
                        required if you are using a masked model or warp-to-
                        landmarks is enabled. Defaults to
                        <input-A>/alignments.json if not provided.
  -alb ALIGNMENTS_PATH_B, --alignments-B ALIGNMENTS_PATH_B
                        Path to alignments file for training set B. Only
                        required if you are using a masked model or warp-to-
                        landmarks is enabled. Defaults to
                        <input-B>/alignments.json if not provided.
  -m MODEL_DIR, --model-dir MODEL_DIR
                        Model directory. This is where the training data will
                        be stored.
  -t {dfaker,dfl-h128,iae,lightweight,original,realface,unbalanced,villain}, --trainer {dfaker,dfl-h128,iae,lightweight,original,realface,unbalanced,villain}
                        Select which trainer to use. Trainers can beconfigured
                        from the edit menu or the config folder.
                          - original: The original model created by
                            /u/deepfakes.
                          - dfaker: 64px in/128px out model from dfaker.
                            Enable 'warp-to-landmarks' for full dfaker method.
                          - dfl-h128. 128px in/out model from deepfacelab
                          - iae: A model that uses intermediate layers to try
                            to get better details
                          - lightweight: A lightweight model for low-end
                            cards. Don't expect great results. Can train as
                            low as 1.6GB with batch size 8.
                          - realface: Customizable in/out resolution model
                            from andenixa. The autoencoders are unbalanced so
                            B>A swaps won't work so well. Very configurable.
                          - unbalanced: 128px in/out model from andenixa. The
                            autoencoders are unbalanced so B>A swaps won't
                            work so well. Very configurable.
                          - villain: 128px in/out model from villainguy. Very
                            resource hungry (11GB for batchsize 16). Good for
                            details, but more susceptible to color
                            differences.
  -s SAVE_INTERVAL, --save-interval SAVE_INTERVAL
                        Sets the number of iterations before saving the model
  -bs BATCH_SIZE, --batch-size BATCH_SIZE
                        Batch size, as a power of 2 (64, 128, 256, etc)
  -it ITERATIONS, --iterations ITERATIONS
                        Length of training in iterations.
  -g GPUS, --gpus GPUS  Number of GPUs to use for training
  -ps PREVIEW_SCALE, --preview-scale PREVIEW_SCALE
                        Percentage amount to scale the preview by.
  -p, --preview         Show preview output. If not specified, write progress
                        to file
  -w, --write-image     Writes the training result to a file even on preview
                        mode
  -ag, --allow-growth   Sets allow_growth option of Tensorflow to spare memory
                        on some configs
  -nl, --no-logs        Disables TensorBoard logging. NB: Disabling logs means
                        that you will not be able to use the graph or analysis
                        for this session in the GUI.
  -pp, --ping-pong      Enable ping pong training. Trains one side at a time,
                        switching sides at each save iteration. Training will
                        take 2 to 4 times longer, with about a 30%-50%
                        reduction in VRAM useage. NB: Preview won't show until
                        both sides have been trained once.
  -msg, --memory-saving-gradients
                        Trades off VRAM useage against computation time. Can
                        fit larger models into memory at a cost of slower
                        training speed. 50%-150% batch size increase for
                        20%-50% longer training time. NB: Launch time will be
                        significantly delayed. Switching sides using ping-pong
                        training will take longer.
  -wl, --warp-to-landmarks
                        Warps training faces to closely matched Landmarks from
                        the opposite face-set rather than randomly warping the
                        face. This is the 'dfaker' way of doing warping.
                        Alignments files for both sets of faces must be
                        provided if using this option.
  -nf, --no-flip        To effectively learn, a random set of images are
                        flipped horizontally. Sometimes it is desirable for
                        this not to occur. Generally this should be left off
                        except for during 'fit training'.
  -tia TIMELAPSE_INPUT_A, --timelapse-input-A TIMELAPSE_INPUT_A
                        For if you want a timelapse: The input folder for the
                        timelapse. This folder should contain faces of A which
                        will be converted for the timelapse. You must supply a
                        --timelapse-output and a --timelapse-input-B
                        parameter.
  -tib TIMELAPSE_INPUT_B, --timelapse-input-B TIMELAPSE_INPUT_B
                        For if you want a timelapse: The input folder for the
                        timelapse. This folder should contain faces of B which
                        will be converted for the timelapse. You must supply a
                        --timelapse-output and a --timelapse-input-A
                        parameter.
  -to TIMELAPSE_OUTPUT, --timelapse-output TIMELAPSE_OUTPUT
                        The output folder for the timelapse. If the input
                        folders are supplied but no output folder, it will
                        default to your model folder /timelapse/

////////////////

usage: faceswap.py convert [-h] [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE]
                           -i INPUT_DIR -o OUTPUT_DIR [-al ALIGNMENTS_PATH]
                           [-l REF_THRESHOLD] [-n NFILTER [NFILTER ...]]
                           [-f FILTER [FILTER ...]] -m MODEL_DIR
                           [-a INPUT_ALIGNED_DIR] [-ref REFERENCE_VIDEO]
                           [-c {none,avg-color,color-transfer,match-hist,seamless-clone}]
                           [-sc {none,sharpen}]
                           [-M {components,dfl_full,facehull,none,predicted}]
                           [-w {ffmpeg,gif,opencv,pillow}] [-osc OUTPUT_SCALE]
                           [-g GPUS] [-fr FRAME_RANGES [FRAME_RANGES ...]]
                           [-k] [-s] [-sp]
                           [-t {dfaker,dfl-h128,iae,lightweight,original,realface,unbalanced,villain}]

Convert a source image to a new one with the face swapped

optional arguments:
  -h, --help            show this help message and exit
  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}
                        Log level. Stick with INFO or VERBOSE unless you need
                        to file an error report. Be careful with TRACE as it
                        will generate a lot of data
  -LF LOGFILE, --logfile LOGFILE
                        Path to store the logfile. Leave blank to store in the
                        faceswap folder
  -i INPUT_DIR, --input-dir INPUT_DIR
                        Input directory or video. Either a directory
                        containing the image files you wish to process or path
                        to a video file.
  -o OUTPUT_DIR, --output-dir OUTPUT_DIR
                        Output directory. This is where the converted files
                        will be saved.
  -al ALIGNMENTS_PATH, --alignments ALIGNMENTS_PATH
                        Optional path to an alignments file.
  -l REF_THRESHOLD, --ref_threshold REF_THRESHOLD
                        Threshold for positive face recognition. For use with
                        nfilter or filter. Lower values are stricter.
  -n NFILTER [NFILTER ...], --nfilter NFILTER [NFILTER ...]
                        Reference image for the persons you do not want to
                        process. Should be a front portrait. Multiple images
                        can be added space separated
  -f FILTER [FILTER ...], --filter FILTER [FILTER ...]
                        Reference images for the person you want to process.
                        Should be a front portrait. Multiple images can be
                        added space separated
  -m MODEL_DIR, --model-dir MODEL_DIR
                        Model directory. A directory containing the trained
                        model you wish to process.
  -a INPUT_ALIGNED_DIR, --input-aligned-dir INPUT_ALIGNED_DIR
                        Input "aligned directory". A directory that should
                        contain the aligned faces extracted from the input
                        files. If you delete faces from this folder, they'll
                        be skipped during conversion. If no aligned dir is
                        specified, all faces will be converted
  -ref REFERENCE_VIDEO, --reference-video REFERENCE_VIDEO
                        Only required if converting from images to video.
                        Provide The original video that the source frames were
                        extracted from (for extracting the fps and audio).
  -c {none,avg-color,color-transfer,match-hist,seamless-clone}, --color-adjustment {none,avg-color,color-transfer,match-hist,seamless-clone}
                        Performs color adjustment to the swapped face. Some of
                        these options have configurable settings in
                        '/config/convert.ini' or 'Edit > Configure Convert
                        Plugins':
                          - avg-color: Adjust the mean of each color channel
                            in the swapped reconstruction to equal the mean of
                            the masked area in the orginal image.
                          - color-transfer: Transfers the color distribution
                            from the source to the target image using the mean
                            and standard deviations of the L*a*b* color space.
                          - match-hist: Adjust the histogram of each color
                            channel in the swapped reconstruction to equal the
                            histogram of the masked area in the orginal image.
                          - seamless-clone: Use cv2's seamless clone function
                            to remove extreme gradients at the mask seam by
                            smoothing colors. Generally does not give very
                            satisfactory results.
                          - none: Don't perform color adjustment.
  -sc {none,sharpen}, --scaling {none,sharpen}
                        Performs a scaling process to attempt to get better
                        definition on the final swap. Some of these options
                        have configurable settings in '/config/convert.ini' or
                        'Edit > Configure Convert Plugins':
                          - sharpen: Perform sharpening on the final face.
                          - none: Don't perform any scaling operations.
  -M {components,dfl_full,facehull,none,predicted}, --mask-type {components,dfl_full,facehull,none,predicted}
                        Mask to use to replace faces. Blending of the masks
                        can be adjusted in '/config/convert.ini' or 'Edit >
                        Configure Convert Plugins':
                          - components: An improved face hull mask using a
                            facehull of 8 facial parts.
                          - dfl_full: An improved face hull mask using a
                            facehull of 3 facial parts.
                          - facehull: Face cutout based on landmarks.
                          - predicted: The predicted mask generated from the
                            model. If the model was not trained with a mask
                            then this will fallback to 'dfl_full'
                          - none: Don't use a mask.
  -w {ffmpeg,gif,opencv,pillow}, --writer {ffmpeg,gif,opencv,pillow}
                        The plugin to use to output the converted images. The
                        writers are configurable in '/config/convert.ini' or
                        `Edit > Configure Convert Plugins:'
                          - ffmpeg: [video] Writes out the convert straight to
                            video. When the input is a series of images then
                            the '-ref' (--reference-video) parameter must be
                            set.
                          - gif: [animated image] Create an animated gif.
                          - opencv: [images] The fastest image writer, but
                            less options and formats than other plugins.
                          - pillow: [images] Slower than opencv, but has more
                            options and supports more formats.
  -osc OUTPUT_SCALE, --output-scale OUTPUT_SCALE
                        Scale the final output frames by this amount. 100%
                        will output the frames at source dimensions. 50% at
                        half size 200% at double size
  -g GPUS, --gpus GPUS  Number of GPUs to use for conversion
  -fr FRAME_RANGES [FRAME_RANGES ...], --frame-ranges FRAME_RANGES [FRAME_RANGES ...]
                        frame ranges to apply transfer to e.g. For frames 10
                        to 50 and 90 to 100 use --frame-ranges 10-50 90-100.
                        Files must have the frame-number as the last number in
                        the name! Frames falling outside of the selected range
                        will be discarded unless '-k' (--keep-unchanged) is
                        selected.
  -k, --keep-unchanged  When used with --frame-ranges outputs the unchanged
                        frames that are not processed instead of discarding
                        them.
  -s, --swap-model      Swap the model. Instead of A -> B, swap B -> A
  -sp, --singleprocess  Disable multiprocessing. Slower but less resource
                        intensive.
  -t {dfaker,dfl-h128,iae,lightweight,original,realface,unbalanced,villain}, --trainer {dfaker,dfl-h128,iae,lightweight,original,realface,unbalanced,villain}
                        [LEGACY] This only needs to be selected if a legacy
                        model is being loaded or if there are multiple models
                        in the model folder



